{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ExcpnfEj7GQ"
      },
      "source": [
        "# Tutorial T1 - DATA PREPARATION\n",
        "\n",
        "The aim of this tutorial is to explain and to illustrate the different steps needed when reading and preparing already collected data. \n",
        "\n",
        "We are using a dataset about the life expectancy in different countries based on data collected by the World Health Organization (WHO) as presented in a Kaggle-project, see https://www.kaggle.com/kumarajarshi/life-expectancy-who\n",
        "\n",
        "Kaggle (www.kaggle.com) is an excellent forum for different data science projects. Different tasks are shown and discussed based on the data provided. Everyone can contribute and participate. The discussions and notebooks presented are in line with the content of this course.\n",
        "\n",
        "Kaggle also contains tutorials to the different tools that we are using in the lab, which are:\n",
        "- scikit-learn with Python, Pandas, matplotlib, Jupyter Notebook, etc.\n",
        "\n",
        "\n",
        "The content and structure of this tutorial is as follows:\n",
        "\n",
        "- 1. Reading and importing data into DataFrames\n",
        "  - 1.1 Reading data from comma separated textfiles: pandas.read_csv()\n",
        "  - 1.2 Reading selected data from data sources\n",
        "- 2. DataFrames\n",
        "  - 2.1 Accessing rows, columns and cells in a DataFrame\n",
        "    - 2.1.1 Splicing\n",
        "    - 2.1.2 Masking\n",
        "  - 2.2 Defining own DataFrames\n",
        "    - 2.2.1 Copying parts of DataFrames\n",
        "    - 2.2.2 Renaming column names\n",
        "    - 2.2.3 Setting a column as index\n",
        "    - 2.2.4 Adding new columns\n",
        "- 3. Manipulating Data\n",
        "  - 3.1 Finding and handling missing values\n",
        "    - 3.1.1 Deleting all the rows with missing values\n",
        "    - 3.1.2 Setting missing values to a fixed value\n",
        "    - 3.1.3 Interpolation within the values of a column\n",
        "    - 3.1.4 Removing columns with missing values\n",
        "    - 3.1.5 Removing the rest of missing data in the Life Expectancy data set\n",
        "  - 3.2. Correcting wrong values\n",
        "  - 3.3. Checking and converting data types\n",
        "  - 3.4 Solving problems with thousand separators\n",
        "  - 3.5 Converting Categorical Data\n",
        "    - 3.5.1 Label Binarizer\n",
        "    - 3.5.2 LabelEncoder\n",
        "    - 3.5.3 OrdinalEncoder\n",
        "    - 3.5.4 Dummy Variables \n",
        "  - 3.6 Aggregation of data\n",
        " - 4. Saving the Pre-Processed Data\n",
        "   - 4.1 Pickling and Unpickling Data\n",
        "   - 4.2 Saving the resulting dataset in excel\n",
        "\n",
        "## How to work with this tutorial\n",
        "This is a Jupyter Notebook that combines narrative text with executable Python code. If you are unfamiliar with notebooks, you might want to first read about how to use it here: https://www.dataquest.io/blog/jupyter-notebook-tutorial/\n",
        "\n",
        "Please work through this tutorial cell by cell and row by row. If a cell contains code you should look at the code and try to understand it before executing it by either pressing the 'Run'-button at the top or the run-symbol to the left of the cell or 'Shift'-'Enter' on the keyboard. If a row only contains text it explains the reasoning and the results behind the code. Try to understand as much as possible, use the links to more information if you need more examples and explanations. Read the course material according to the reading instructions, especially if Jupyter, NumPy and Pandas is new to you.\n",
        "The goal of our tutorials is to give you concrete examples of how to use the commands and functions from the different libraries for the kind of tasks that you will need to apply data science in the lab-exercises.\n",
        "\n",
        "The notebook structure allows you to 'play with code'! You can copy existing cells, you can add new cells and you can change code in the examples to see what happens. This is the best way to gain understanding and to test if you grasp the concepts. So, please play and don't be shy, no one needs to see what you are doing with this tutorial, you will not turn it in neither. At some places in this tutorial we will even ask you to write your own code so that you can see if you have understood. We indicate these places with the phrase: \n",
        "### Your turn:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MmRarv5Fj7GW"
      },
      "outputs": [],
      "source": [
        "# Import all libraries needed for this tutorial\n",
        "\n",
        "# Pandas: used to create and work with DataFrames\n",
        "import pandas as pd #imports all of pandas library\n",
        "from pandas import DataFrame, read_csv #imports some of the important functions from the pandas library.\n",
        "\n",
        "# Scikit-learn: import preprocessing functions\n",
        "from sklearn.preprocessing import LabelEncoder, label_binarize, OrdinalEncoder\n",
        "\n",
        "# Saving variables as files on disk\n",
        "import pickle\n",
        "\n",
        "### Section defining File Paths - e.g. the path to your files, either on Colab or on your own computer\n",
        "inputFilePath='/drive/My Drive/Colab Notebooks/MA661E/'  #'' \n",
        "outputFilePath='/drive/My Drive/Colab Notebooks/MA661E/'  #''\n",
        "# mounting google drive if working in Colab - if not working on Colab do comment-out the following two rows\n",
        "from google.colab import drive\n",
        "drive.mount('/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kao6-l96j7GX"
      },
      "source": [
        "Start by reading and looking at the Kaggle site https://www.kaggle.com/kumarajarshi/life-expectancy-who. Then download the .csv-file from the kaggle-site or from Canvas to your computer. Start excel and import the .csv-file as a textfile with headers. One way is to start with an empty excel sheet and to chose 'import'->from file. If you succeed with the import, your data will be in an excel-sheet and you can save it as an .xlsx-file. Excel is a very good tool to look at data and can be used to analyze data as well. But we will learn here how to create pandas DataFrames with data available in text-files or excel-tables instead.\n",
        "\n",
        "Save the files LifeExpectancyData.csv and LifeExpectancyData.xlsx in the same folder as this jupyter notebook.\n",
        "\n",
        "In the header of the text-file you should see the names of the different columns which also are called 'fields', 'features', 'attributes', 'variables'. One of the discussions avaiable in Kaggle contains a better explanation of the different fields:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ghxd9Xqvj7GY"
      },
      "source": [
        "|Field|Description|\n",
        "|---:|:---|\n",
        "|Country|Country|\n",
        "|Year|Year|\n",
        "|Status|Developed or Developing status|\n",
        "|Life expectancy|Life Expectancy in age|\n",
        "|Adult Mortality|Adult Mortality Rates of both sexes (probability of dying between 15 and 60 years per 1000 population)|\n",
        "|infant deaths|Number of Infant Deaths per 1000 population|\n",
        "|Alcohol|Alcohol, recorded per capita (15+) consumption (in litres of pure alcohol)|\n",
        "|percentage expenditure|Expenditure on health as a percene of Gross Domestic Product per capita(%)|\n",
        "|Hepatitis B|Hepatitis B (HepB) immunization coverage among 1-year-olds (%)|\n",
        "|Measles|Measles - number of reported cases per 1000 population|\n",
        "|BMI|Average Body Mass Index of entire population|\n",
        "|under-five deaths|Number of under-five deaths per 1000 population|\n",
        "|Polio|Polio (Pol3) immunization coverage among 1-year-olds (%)|\n",
        "|Total expenditure|General government expenditure on health as a percentage of total government expenditure (%)|\n",
        "|Diphtheria|Diphtheria tetanus toxoid and pertussis (DTP3) immunization coverage among 1-year-olds (%)|\n",
        "|HIV/AIDS|Deaths per 1 000 live births HIV/AIDS (0-4 years)|\n",
        "|GDP|Gross Domestic Product per capita (in USD)|\n",
        "|Population|Population of the country|\n",
        "|thinness 1-19 years|Prevalence of thinness among children and adolescents for Age 10 to 19 (%)|\n",
        "|thinness 5-9 years|Prevalence of thinness among children for Age 5 to 9(%)|\n",
        "|Income composition of resources|Income composition of resources|\n",
        "|Schooling|Number of years of Schooling(years)|"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8qk_vi1j7GY"
      },
      "source": [
        "## 1. Reading and importing data into DataFrames\n",
        "\n",
        "The Pandas library contains high-level functions to import data from different sources (such as databases, web-pages) or to read data from files, see the overview at https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "898gJM3Zj7GZ"
      },
      "source": [
        "### 1.1 Reading data from comma separated textfiles: pandas.read_csv()\n",
        "- Since the Kaggle-data is available as comma separated textfile, we can use the reading function read_csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCY1Xl0zj7GZ"
      },
      "source": [
        "If you are using Googles Colab you will need to copy this Jupyter notebook as well as the data-files LifeExpectancyData.csv and LifeExpectancyData.xlsx into the dedicated folder on your Google Drive. Change the next cell from 'Markdown' to 'Code' in order to rum it!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "pGPxeNhXj7Ga"
      },
      "outputs": [],
      "source": [
        "#loading data from the csv-textfile into a pandas DataFrame-variable rawdata\n",
        "# make sure to have the file 'LifeExpectancyData.csv' in your inputFilePath\n",
        "rawdata=pd.read_csv(inputFilePath+'LifeExpectancyData.csv')\n",
        "# showing the first 5 records\n",
        "rawdata.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fODPO9Ej7Gc"
      },
      "source": [
        "### 1.2 Reading selected data from data sources\n",
        "The Pandas reader functions, such as read_csv, are very powerful and allow many different alterations and corrections already during the data-import into DataFrames. Examples include the choice of separation signs, the selection of columns, adding or changing column names, eliminating missing values, defining quoting escape-characters, and thousand separators, handling date conversions, etc. Sometimes the data contains no headers, and the name of the fields or columns are described in another file. The read_csv function allows to define the list of column names which is very convenient and useful.\n",
        "\n",
        "Take a look at the function description from the Pandas API reference: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html#pandas.read_csv\n",
        "\n",
        "Since most of these functions can be performed even after the import with help of DataFrames functionality, we will illustrate them under the DataFrame section. Working with DataFrames is more transparent than working with data files but less efficient. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UiT7MJ8Bj7Gd"
      },
      "outputs": [],
      "source": [
        "#Example: Using the read_csv import function on the file 'LifeExpectancyData.csv' to import \n",
        "#the first 4 columns and translate their field names to Swedish.\n",
        "\n",
        "exData=pd.read_csv(inputFilePath+'LifeExpectancyData.csv', header=0, usecols=[0,1,2,3],names=['Land','År','Status','Livslängd'])\n",
        "# showing the first 5 records\n",
        "exData.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFaARpZjj7Gd"
      },
      "source": [
        "### Your Turn:\n",
        "- You might encounter some of the typical problems of importing data when working with the Kaggle project that you selected, like:\n",
        "    - automatically detect separator signs \n",
        "    - wanting to import numbers as int or floats\n",
        "    - removing thousend-separators in numbers\n",
        "    - selecting only a subset of columns, wanting to rename column names\n",
        "    - importing dates as dates\n",
        "Experiment with your data and the pd.read_csv() function to learn how to solve any of the problems listed above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S7wDKKOCj7Gd"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SgHCoEx6j7Gd"
      },
      "source": [
        "## 2. DataFrames\n",
        "DataFrames are special variables in Pandas that allow to store vectors and Matrices. Vectors are like an array of values and matrices are arrays of arrays, organized in rows and columns.Think of a data-sheet in excel, then the rows and columns are 'vectors' and the whole sheet or table is a 'matrix'.\n",
        "If you are familiar with Matlab, you will recognize the similarity of DataFrames with Matlab's matrix variables.\n",
        "\n",
        "A Pandas reference to the DataFrame API can be found here: https://pandas.pydata.org/pandas-docs/stable/reference/frame.html\n",
        "\n",
        "\n",
        "We will start by looking at the most important functionalities of DataFrames, such as:\n",
        "- How to access rows and columns in DataFrames\n",
        "- How to copy parts of DataFrames\n",
        "- How to change column names and data types\n",
        "- How to add columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l38QCGmPj7Ge"
      },
      "source": [
        "### 2.1 Accessing rows, columns and cells in a DataFrame\n",
        "The proper way to address columns, rows and cells in a matrix is to use indexes. Mathematically, one for example writes A[i,j] to address an element in the matrix A that is located in the i-th row and j-th column. When addressing the entire i-th row one can write A[i,:], and similarly, A[:,j] corresponds to the entire j-th column.\n",
        "\n",
        "With DataFrame one can use the function .iloc[] for the same thing. Be aware that the range of index for rows and columns (called axis) start with '0' and end with length-1. See the description of iloc[] here: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.iloc.html\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J-WX9VdNj7Ge"
      },
      "outputs": [],
      "source": [
        "#Examples of addressing cells\n",
        "print(rawdata.iloc[0,0], rawdata.iloc[100,12])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rl5a8z66j7Ge"
      },
      "source": [
        "When addressing rows, columns and cells by numeric indexes it is important to keep the indexes within the dimensions of the DataFrame. Use the attribute 'shape' of a DataFrame to get information about the number of rows and columns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8_4AqTzqj7Ge"
      },
      "outputs": [],
      "source": [
        "# Example returning 2938 rows and 22 columns\n",
        "rawdata.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yxqx61irj7Gf"
      },
      "source": [
        "Use a ':' when selecting the entire column or row:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MO9YUqCxj7Gf"
      },
      "outputs": [],
      "source": [
        "#example of selecting the first column\n",
        "rawdata.iloc[:,0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TdcXYcFbj7Gf"
      },
      "outputs": [],
      "source": [
        "#example of selecting the third row\n",
        "rawdata.iloc[2,:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qoj-wA-ij7Gf"
      },
      "source": [
        "An important advantage of DataFrames is the possibility of having labels, both - for indexes (rows) as well as column names. Single columns can directly be addressed by its name, either in list-form (['column-name'], or in case the name does not contain spaces, -even as a attribute or dict-key (.cname)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "p3gx8Q-Ij7Gf"
      },
      "outputs": [],
      "source": [
        "#example how to select the column 'GDP'\n",
        "rawdata['GDP']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cgg5p-_Ej7Gg"
      },
      "outputs": [],
      "source": [
        "rawdata.GDP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mbj0C9vLj7Gg"
      },
      "source": [
        "Selecting columns by their names requires that the name is spelled correctly, including right number of spaces. It is therefore a good idea to list all of the column names with the attribute 'columns'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2TUHpWHvj7Gg"
      },
      "outputs": [],
      "source": [
        "# Example: Listing all column names of rawdata. Observe the spaces in several of the names!\n",
        "rawdata.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A7ShF1eaj7Gg"
      },
      "source": [
        "If you try, you will see that you can not add several column names as list or select some rows. This requires the use of the .loc[] function. It allows you to select rows by numbers and columns by names. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wAnimkjRj7Gg"
      },
      "outputs": [],
      "source": [
        "#Example of selecting the third row of the columns GDP and Schooling\n",
        "rawdata.loc[2,['GDP','Schooling']]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7HTDCtrgj7Gh"
      },
      "source": [
        "#### 2.1.1 Splicing\n",
        "One can also use intervals for the selection of rows and columns:\n",
        "- For iloc[] the intervals are numeric in the form a:b, with a, b being integers so that a<b. The slices will be from a to b-1.\n",
        "- For loc[], the intervals for the rows depend on the index. If the index is the default one it will be numeric like for iloc[]. But unlike for iloc, the slices will be from row a to row b The interval for the columns can use the names of the columns instead. Observe that the names given indicate the first and last column in the interval!\n",
        "- Intervals can be designed in both orientations: left to right from first to last values, or right to left from last to first. The use of the minus-sign '-' indicates the 'reversed' order: -4, for example, means the forth-last element."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LAhbc0RJj7Gh"
      },
      "outputs": [],
      "source": [
        "#Example of slices with iloc[]: - first three rows of columns 2-5\n",
        "rawdata.iloc[0:3,2:6]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vFaRG8sIj7Gh"
      },
      "outputs": [],
      "source": [
        "#Example of slices with loc[]: - first three rows of columns from Status to Infant deaths\n",
        "rawdata.loc[0:2,'Status':'infant deaths']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NPh4y_moj7Gh"
      },
      "source": [
        "#### 2.1.2 Masking\n",
        "One can use normal conditions as well as boolesk expressions as part of the index-descriptions in order to find data that fulfills these conditions and expressions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "osfgxGhzj7Gi"
      },
      "outputs": [],
      "source": [
        "#Example: Find all the information about Sweden\n",
        "rawdata[rawdata['Country']=='Sweden']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "yHfe84UAj7Gi"
      },
      "outputs": [],
      "source": [
        "#Example: same condition as part of a .loc[]\n",
        "rawdata.loc[rawdata['Country']=='Sweden',:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A-AMtl3Ij7Gi"
      },
      "outputs": [],
      "source": [
        "# Example: Combining conditions with logical expressions\n",
        "rawdata[(rawdata['Country']=='Sweden') & (rawdata['Year']==2013) ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VpHSDXC8j7Gi"
      },
      "outputs": [],
      "source": [
        "# Example: Combining conditions with logical expressions with list of values\n",
        "rawdata[(rawdata['Country']=='Sweden') & (rawdata['Year'].isin([2013,2015]))]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B3o4mwJoj7Gi"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UkpVF0z7j7Gi"
      },
      "source": [
        "### 2.2 Defining own DataFrames \n",
        "The rawdata that we imported above contains 22 columns or fields. Usually, one is only interested in a few of these columns. By selecting rows and columns as showed by the examples above, one can customize the data one wants to work with.\n",
        "Usually one creates a new dataFrame variable as a copy of the selection using the function copy(). If one creates a new variable without copying the selection, a reference to the original data is being created. Changes made to the new variable will then also be made to the original. This can be useful if the entire data set is too large to work with. Instead, one can reference to different parts of the data sequentially."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gfOmYaAhj7Gj"
      },
      "source": [
        "#### 2.2.1 Copying parts of DataFrames "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "bl1Tw6zVj7Gj"
      },
      "outputs": [],
      "source": [
        "# Example: Defining a new dataFrame variable 'dFSelection' that consists of Country, Year, Status, Schooling and Life expectancy\n",
        "dFSelection=rawdata.loc[:,['Country', 'Year', 'Status', 'Schooling','Life expectancy ']].copy()\n",
        "dFSelection.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zy4MegLpj7Gj"
      },
      "source": [
        "Some important things to notice in the example above:\n",
        "- copy() generates a copy of the selection instead of a reference or view to the original data\n",
        "- .loc is needed with a list of column names\n",
        "- ':' returns all the rows\n",
        "- the order of the columns correspond with the order of the list of the column names and not with the order in the original data\n",
        "- Column names in the list must be spelled the way they are in the original data, otherwise new columns are created. Use the attribute 'columns' for a list of column names and their spellings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "chL0mZSYj7Gj"
      },
      "outputs": [],
      "source": [
        "rawdata.columns\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9NZ5ePrmj7Gj"
      },
      "source": [
        "#### 2.2.2 Renaming column names\n",
        "In the example above, several column names contain empty spaces at the end as well as a between the words. This complicates accessing the column unneccessarily. 'Thinness 1-19 years' should actually read 10-19 years. We can use the function rename() to change the column names."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "ztImW3Jvj7Gj"
      },
      "outputs": [],
      "source": [
        "# Example: Renaming the column names to eliminate spaces.\n",
        "dfLE=rawdata.copy() #copying the rawdata\n",
        "\n",
        "dfLE=dfLE.rename(columns={'Life expectancy ':'Life_expectancy',\n",
        "                 'Adult Mortality':'Adult_Mortality',\n",
        "                 'infant deaths':'infant_deaths',\n",
        "                 'percentage expenditure':'percentage_expenditure',\n",
        "                 'Hepatitis B':'Hepatitis_B',\n",
        "                 'Measles ':'Measles',\n",
        "                 ' BMI ':'BMI',\n",
        "                 'under-five deaths ':'under-five_deaths',\n",
        "                 'Total expenditure':'Total_expenditure',\n",
        "                 'Diphtheria ':'Diphtheria',\n",
        "                 ' thinness  1-19 years':'thinness_10-19_years',\n",
        "                 ' thinness 5-9 years':'thinness_5-9_years',\n",
        "                 'Income composition of resources':'Income_composition_resources'}\n",
        "                )\n",
        "dfLE.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PkeMusYJj7Gk"
      },
      "outputs": [],
      "source": [
        "dfLE.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gz2ryddUj7Gk"
      },
      "source": [
        "#### 2.2.3 Setting a column as index\n",
        "By default, the index for the rows are auto-incremented integer values, starting with zero (0). Any column can be used as row-index instead. A tutorial about indexing, selecting and assigning in DataFrames can be found here:\n",
        "https://www.kaggle.com/residentmario/indexing-selecting-assigning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QeeozLbbj7Gk"
      },
      "outputs": [],
      "source": [
        "# Example: Setting the countries as index for the rows\n",
        "dFCountries=dFSelection.set_index('Country')\n",
        "dFCountries.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Khp2TchVj7Gk"
      },
      "source": [
        "- Country is used as index. As you can see, indexes do not need to be unique\n",
        "- Index can be used to select rows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5bdO7nI9j7Gk"
      },
      "outputs": [],
      "source": [
        "# Example: Selecting all data from 'Sweden'\n",
        "dFCountries.loc['Sweden',:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2cVKG4Rj7Gk"
      },
      "source": [
        "#### 2.2.4 Adding new columns\n",
        "New columns are simply added by pretending that the column already exists and will receive a new value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MC96oS13j7Gl"
      },
      "outputs": [],
      "source": [
        "dFCountries.columns\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q9o4hxw6j7Gl"
      },
      "outputs": [],
      "source": [
        "# Example: Adding a new column 'LifeSchooling' which is the ratio of life expectancy divided by average years of schooling\n",
        "dFCountries['LifeSchooling']=dFCountries['Life expectancy ']/dFCountries.Schooling\n",
        "dFCountries.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TY6Bq20zj7Gl"
      },
      "source": [
        "## 3. Manipulating Data\n",
        "Often, the data collected needs to be 'purified' from different kinds of impurities. Typical such problems are missing data, wrong data, special characters used for quotes, decimal points and thousand separators. Even the conversion of special data types, like dates or floating point values, unicode text, etc. are material for nightmares. Since most of the statistical methods and machine learnnig algorithms work purely on numeric data one usually also has to convert categorical data into numerical."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K2awOpHSj7Gl"
      },
      "source": [
        "### 3.1 Finding and handling missing values\n",
        "\n",
        "The way to find out how many missing values for each column there are is to use the .isnull() or .isna() function on DataFrames. They usually generate the same results, which allows the interpretation that 'null' and 'not a number (NaN)' are considered to be the same within DataFrames. These functions return a boolean value 'True' if a value is missing or 'False' if it is not missing. By summing upp all the 'Trues', meaning all the missing values, one can get the total of missing values per column or field or feature."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FZdA95rlj7Gl"
      },
      "outputs": [],
      "source": [
        "# Example: Finding missing values in the original data\n",
        "rawdata.isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hrnYhVMtj7Gl"
      },
      "source": [
        "As you can see, there are quite a few missing values, (the total can easely be calculated by summing up the sums with rawdata.isnull().sum().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R3XHxVsrj7Gm"
      },
      "outputs": [],
      "source": [
        "rawdata.isnull().sum().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_2FHM-sdj7Gm"
      },
      "source": [
        "Hence, of 2938 records up to 2563 can contain missing values, in the worst case that is. We do not know how the missing values are distributed over the records or rows. In the worst case there is one missing value per row, e.g. 2563 rows. Most likely there will be more than one missing value per row.\n",
        "\n",
        "Unfortunately, most computer applications cannot deal with such missing values or produce strange results.\n",
        "So the question is how to handle these missing values. Different scenarioes are possible:\n",
        "- One way is to delete the whole row whenever there is a missing value present\n",
        "- Another possibility is to set missing values to a fixed value, for example '0'. But putting an unknown value to '0' can be quite wrong, for example if it is the value for the polulation of a country.\n",
        "- Instead of '0' or some other fixed value, one could set it to the mean value over all existing measurements or as the average value compared to values before and after. However, this only makes sense if the values to interpolate with are grouped according to 'Country'. Grouping is possible but interpolation over groups is rather complicated.\n",
        "- Missing values, just like wrong values could be corrected by finding out the right values.\n",
        "- If one column has many missing values one can argue that this field or feature is rather useless and can be dropped alltogether. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJ3Nrzryj7Gm"
      },
      "source": [
        "#### 3.1.1 Deleting all the rows with missing values\n",
        "If much data is available and the columns are important, the easiest way to deal with missing data is to remove all of the rows that are not complete. This is what the function 'dropna()' does."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D9AvDRCJj7Gm"
      },
      "outputs": [],
      "source": [
        "# Example: Deleting all the rows with missing data\n",
        "dFDropna=rawdata.copy() #making coyp of the original data\n",
        "print(dFDropna.shape)#dimension of the data before removing rows\n",
        "dFDropna=dFDropna.dropna() #need to save results\n",
        "print(dFDropna.shape)#dimension of the data after removing rows\n",
        "print('Total number of missing values after dropna: ',dFDropna.isnull().sum().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9yaD7w84j7Gm"
      },
      "source": [
        "- 1289 rows had to be removed in order to eliminate the 2563 missing values!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHbh3kqlj7Gm"
      },
      "source": [
        "#### 3.1.2 Setting missing values to a fixed value\n",
        "Sometimes, it might make sense to set unknown values to zero or to another fixed value, for example a minimum, maximum or average value.\n",
        "The function 'fillna()' fills missing values with fixed values, see https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.fillna.html?highlight=fillna#pandas.DataFrame.fillna\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ry5O_hHsj7Gm"
      },
      "outputs": [],
      "source": [
        "# Example: Fill the missing values for 'Diphtheria ' with zeroes.\n",
        "dFDip=rawdata.copy() #making a copy of the original data\n",
        "print('Number of missing data in Diphteria: ', dFDip['Diphtheria '].isnull().sum())\n",
        "dFDip['Diphtheria '].fillna(value=0, inplace=True)# inplace=True means changes are saved in the dataFrame\n",
        "print('Number of missing data in Diphteria: ', dFDip['Diphtheria '].isnull().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iPGZl0xQj7Gm"
      },
      "outputs": [],
      "source": [
        "# Example: Fill the missing values for 'Diphtheria ' and Hepatitis_B with zeroes.\n",
        "\n",
        "dfLE['Diphtheria'].fillna(value=0, inplace=True)# inplace=True means changes are saved in the dataFrame\n",
        "dfLE['Hepatitis_B'].fillna(value=0, inplace=True)\n",
        "print('Number of missing data in Diphteria: ', dfLE['Diphtheria'].isnull().sum())\n",
        "print('Number of missing data in Hepatitis_B: ', dfLE['Hepatitis_B'].isnull().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LitAGxJlj7Gn"
      },
      "source": [
        "#### 3.1.3 Interpolation within the values of a column\n",
        "To interpolate unknown values based on known values is usually a good idea, but can be tricky. Usually one would like to interpolate based on similar values, e.g. measurements conducted close in time, values belonging to a cluster of similar data points, etc. To define on which values to interpolate is usually not that easy. Here, we only show the easiest way of interpolating on the values of the entire column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hxr9AbkPj7Gn"
      },
      "outputs": [],
      "source": [
        "# Example: Interpolate the missing values for 'Income composition of resources' based on the values in the column\n",
        "print('Number of missing data in Income composition of resources: ', \n",
        "      dfLE['Income_composition_resources'].isnull().sum())\n",
        "dfLE['Income_composition_resources']=dfLE['Income_composition_resources'].interpolate()\n",
        "print('Number of missing data in Income composition of resources: ', \n",
        "      dfLE['Income_composition_resources'].isnull().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ci_7Jj6aj7Gn"
      },
      "source": [
        "#### 3.1.4 Removing columns with missing values\n",
        "If a column is not that important, for example if it contains a lot of missing values, it might as well be removed altogether. A row or a column can be deleted with the drop() function, see https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.drop.html?highlight=drop#pandas.DataFrame.drop\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "0dWbDx8Tj7Gn"
      },
      "outputs": [],
      "source": [
        "# Example: Removing the column 'Polio', assuming that Polio will be extinguished within a generation and therefore \n",
        "# can be removed\n",
        "print('List of columns before removing \"Polio\": ', dfLE.columns)\n",
        "dfLE.drop(columns='Polio', inplace=True)\n",
        "# checking if the column 'Polio' has disappeared from rawdata\n",
        "print('List of columns before removing \"Polio\": ', dfLE.columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZrMGA_JIj7Gn"
      },
      "source": [
        "#### 3.1.5 Removing the rest of missing data in the Life Expectancy data set\n",
        "- We tried to fill the missing data with some values instead of removing entire rows\n",
        "\n",
        "Checking how much missing data that is left before removing it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rkHEQ8qZj7Gn"
      },
      "outputs": [],
      "source": [
        "#Example: Comparing the amount of missing data left with the original amount of missing data\n",
        "# Overview original data (rawdata)\n",
        "print(rawdata.isnull().sum())\n",
        "print('Total number of missing data: ',rawdata.isnull().sum().sum())\n",
        "# Overview pre-processed data (dfLE)\n",
        "print(dfLE.isnull().sum())\n",
        "print('Total number of missing data: ',dfLE.isnull().sum().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZBIPj-kfj7Go"
      },
      "source": [
        "- 758 missing values less\n",
        "- But still 1805 missing values left"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dAJkBqd8j7Go"
      },
      "outputs": [],
      "source": [
        "#Example: The missing values left are removed by removing the rows\n",
        "dfLE=dfLE.dropna()\n",
        "#checking that no missing values are left\n",
        "print(dfLE.isnull().sum())\n",
        "print('Total number of missing data: ',dfLE.isnull().sum().sum())\n",
        "#documenting the number of rows and columns left in the data\n",
        "print('Number of rows, number of columns: ', dfLE.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWbMyzHsj7Go"
      },
      "source": [
        "### 3.2. Correcting wrong values\n",
        "There are quite a few mistakes in the data which are also discussed on Kaggle's project page. It seems like the WHO did not have all the data available or that someone did a bad job putting the data together from different other datasets.\n",
        "For example, different countries that usually are classified as 'developed' countries are missclassified as developing countries. Greece, Canada, France and Finland are mentioned.\n",
        "- We will show how to set the status of the entries for Greece, Canada, France and Finland to 'developed'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X1HlPEp8j7Go"
      },
      "source": [
        "We can filter these countries and look at their values for the field 'Status'.\n",
        "The general principle for finding data that fulfills certain conditions is to:\n",
        "\n",
        "1) ask for the index (usually the row-number) of all the values that fulfill the condition\n",
        "\n",
        "2) use the index to show the whole row or some columns of the row"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O9XzG4Wfj7Go"
      },
      "outputs": [],
      "source": [
        "# Example: In order to get all the rows with Country== Greece, we can use this list as index-list. \n",
        "rawdata[rawdata['Country']=='Greece']\n",
        "# or with loc[]\n",
        "#rawdata.loc[rawdata['Country']=='Greece']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "07llblRJj7Go"
      },
      "outputs": [],
      "source": [
        "# This returns the list of all rows with Country== Greece. The list itself is a DataFrame.\n",
        "# If we only want to check the field 'Status' of the rows with Greece, we can therefore write:\n",
        "rawdata[rawdata['Country']=='Greece']['Status']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_HUD04wAj7Go"
      },
      "source": [
        "We see that the status is wrong and if we decide that it should read 'Developed' instead for 'Developing' we could try something like:\n",
        "rawdata[rawdata['Country']=='Greece']['Status']='Developed'\n",
        "\n",
        "However, Pandas asks us to use \"Try using .loc[row_indexer,col_indexer] = value instead\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AlqNDMxoj7Gp"
      },
      "outputs": [],
      "source": [
        "# Example: Changing the status of Greece from Developing to Developed\n",
        "rawdata.loc[rawdata['Country']=='Greece','Status']='Developed'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "TmTPZG6mj7Gp"
      },
      "outputs": [],
      "source": [
        "# Checking if this worked with the same command as before:\n",
        "rawdata[rawdata['Country']=='Greece']['Status'].head(4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g0emJL2Kj7Gp"
      },
      "source": [
        "We can do the same for the other countries that we know have a wrong status one by one. A faster and easier way is to use a list of the names of these countries and to use the function isin(['a', 'b', 'c'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eikV7s6Wj7Gp"
      },
      "outputs": [],
      "source": [
        "# Example: Changing the status of several countries at once in our pre-processing data frame dfLE\n",
        "dfLE.loc[rawdata['Country'].isin(['Greece','Canada','France', 'Finland']),'Status']='Developed'\n",
        "#testing with Greece\n",
        "dfLE[dfLE['Country']=='Greece']['Status'].head(4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2gCEFhu6j7Gp"
      },
      "source": [
        "### 3.3. Checking and converting data types\n",
        "Transfering data from one operating system, database or application to another can result in errors or unknown data types. \n",
        "This means that the values in entire columns sometimes need to be changed, for example from integers to floating type values or vice versa. Often, ascii-based text is used since this is the type that is most alike in different systems. Problems appear when the text is unicode or when there are separators between the text like in the case of csv-files. Separators can appear at 'wrong' places causing a shift in the columns, for example.\n",
        "\n",
        "We will use the attribute 'dtypes' to list the datatypes of the columns and the function 'astype()' for converting between different types. See also: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.astype.html?highlight=astype#pandas.DataFrame.astype"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ayewJjhLj7Gp"
      },
      "outputs": [],
      "source": [
        "#Example: Listing the data types of the different columns in the data.\n",
        "rawdata.dtypes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HW-L_CPkj7Gp"
      },
      "source": [
        "Most of the fields are already numeric and either of type int64 or float64. The fields that are described with 'object' are not - Country and Status both contain categories: Country in form of the different countries and Status whether a country is 'developing' or 'developed'. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iahYuiGPj7Gp"
      },
      "source": [
        "Some of the fields that are float64 are actually int. It is not neccessary to transform these values, but for the sake of an example we will do this for 'Adult Mortality'. For this to work, no unknown or missing values may be present in the data!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pCHX_M8Yj7Gq"
      },
      "outputs": [],
      "source": [
        "#Example: How to change the type from float to int\n",
        "dfLE=dfLE.astype({'Adult_Mortality': 'int64'})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TnPJV8ONj7Gq"
      },
      "outputs": [],
      "source": [
        "#Check the type of 'Adult_Mortality'\n",
        "dfLE['Adult_Mortality'].dtypes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RCX6IaoHj7Gq"
      },
      "source": [
        "### 3.4 Solving problems with thousand separators\n",
        "High numbers are sometimes written with thousand separators like for example: 2,156,624,900. Before such text can be converted to int64 numbers, the separators (in this example commas) need to be removed first.\n",
        "One easy way to to this is to simply remove all of the commas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RgsHWioQj7Gq"
      },
      "outputs": [],
      "source": [
        "#Creating an example, since we do not have this case in our data\n",
        "ex= DataFrame({'bignum': ['2,156,624,900', '44,768,444', '3,456','498']})\n",
        "ex"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F9gAIQYfj7Gq"
      },
      "outputs": [],
      "source": [
        "# the type of the column bignum is object, meaning text\n",
        "ex.dtypes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49fDtRahj7Gq"
      },
      "source": [
        "It cannot be transformed to int64, (try it!). All of the commas need to be removed first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QSFdNs9Dj7Gq"
      },
      "outputs": [],
      "source": [
        "# Example: replacing ',' (comma) with '' (nothing) in a \n",
        "# replace function. This kind of looking for patterns is called \"regex\"\n",
        "## source: \"pandas.DataFrame.replace\", pandas user guide\n",
        "## source: https://stackoverflow.com/questions/38516481/trying-to-remove-commas-and-dollars-signs-with-pandas-in-python\n",
        "## source: https://en.wikipedia.org/wiki/Regular_expression\n",
        "ex=ex.replace(',', '',regex=True)\n",
        "ex"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UgDZygUOj7Gq"
      },
      "outputs": [],
      "source": [
        "# Now the values can be transformed\n",
        "ex=ex.astype({'bignum': 'int64'})\n",
        "ex"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "enag_8Dpj7Gr"
      },
      "outputs": [],
      "source": [
        "ex.dtypes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SomhDgqDj7Gr"
      },
      "source": [
        "### 3.5 Converting Categorical Data\n",
        "Usually, parts of the data will not be numerical. For example names, addresses, descriptions, are usually in text form. Some of the fields contain text or information that are categories. A category is a description from a list of possible things. Typical examples are the color of a car or its brand, the name of medicins, or the distinction between male and female.\n",
        "Categories are elements in a set or list. They can have many different values, but they are countable. Hence, if listed in a list, each category can be replaced by the ordernumber in the list. This is the principle behind the function \"LabelEncoder\". If the values are binary, meaning they are either 'A' or 'Not A', they can be replaced with '1' for being 'A' and '0' for not being 'A'. This is the idea behind the function label_binarizer().\n",
        "\n",
        "Another way is to create new labels or fieldnames for each of the categories in the list with the values '1' if the category is present and a '0' for all the other categories. That is the principle of 'dummyVariables'.\n",
        "\n",
        "All of these functions solve the problem that categorical data needs to be transformed into numerical values in order to be able to use the data in statistical models or within machine learning.\n",
        "\n",
        "The transformers with 'Label' in the name are thought to be used on one column only. Instead of 'LabelEncoder' one could use the function 'OrdinalEncoder()' which allows to transform several columns at the same time.\n",
        "\n",
        "The dataset on Life expectancy only contains two columns with categorical data: Country and Status. In order to be able to apply each one of the encoders mentioned above, we will make use of a trimmed version of the Nobel Laureates dataset from Kaggle, see: https://www.kaggle.com/nobelfoundation/nobel-laureates for a description of the entire data set and the different columns.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vKCqsEOUj7Gr"
      },
      "source": [
        "#### Preparing and loading the Nobel Laureates dataset\n",
        "- Download the excel file 'NobelLaureates.xlsx' from Canvas and save it in the same folder as this notebook\n",
        "- It contains the following columns from the original data set: Year, Category, Sex, Birth Country, Organization Name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EygxTj07j7Gr"
      },
      "outputs": [],
      "source": [
        "# Loading the Nobel Laureates data 'NobelLaureates.xlsx'\n",
        "Nobel=pd.read_excel(inputFilePath+'NobelLaureates.xlsx')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "9BHdE_IRj7Gr"
      },
      "outputs": [],
      "source": [
        "Nobel.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMgrAq4-j7Gr"
      },
      "source": [
        "Regarding these 5 columns we can observe the following:\n",
        "- 'Year' is already numeric (integer) and does not need to be transformed\n",
        "- 'Sex' could be binary or some few categories. If it is binary, 'Label_binarizer' could be used. If it contains a few categories and the gender distinction is of importance, the transformation into dummy variables could be chosen.\n",
        "- 'Birth Country' and 'Organization Name' both contain several categories. These columns could be converted one by one using 'Label encoder'. Using OrdinalEncoder, several columns can be converted at the same time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KfGosI19j7Gr"
      },
      "source": [
        "Before converting categories, it is a good idea to:\n",
        "- find out how many different categories there are\n",
        "- document the number of data points for each category\n",
        "- decide whether the column with the categories shall be overwritten by the numbers or if a new column shall be created"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2iZKggm-j7Gs"
      },
      "outputs": [],
      "source": [
        "#Example: Finding out about the categories there are with the function 'unique()' as well as the number with nunique()\n",
        "print(Nobel.Category.nunique(),' categories in Category: ',Nobel.Category.unique())\n",
        "print(Nobel.Sex.nunique(), ' categories in Sex: ',Nobel.Sex.unique())\n",
        "print(Nobel['Birth Country'].nunique(), ' categories in Birth Country: ',Nobel['Birth Country'].unique())\n",
        "print(Nobel['Organization Name'].nunique(), ' categories in Organization Name: ',Nobel['Organization Name'].unique())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "hvbDnxQEj7Gs"
      },
      "outputs": [],
      "source": [
        "#Example: Documenting the number of data points for each category with the function value_counts()\n",
        "print('value counts of Category: \\n', Nobel.Category.value_counts(), \"\\n\")\n",
        "print('value counts of Sex: \\n', Nobel.Sex.value_counts(), \"\\n\")\n",
        "print('value counts of Birth Country: \\n', Nobel['Birth Country'].value_counts(), \"\\n\")\n",
        "print('value counts of Organization Name: \\n', Nobel['Organization Name'].value_counts(), \"\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MmtO1t32j7Gs"
      },
      "source": [
        "Data to be converted may not contain missing or undefinde (Nan) values. We simply remove them in our test-data set of Nobel Laureates."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZtesaQeCj7Gs"
      },
      "outputs": [],
      "source": [
        "#Exampel: Checking for missing values\n",
        "Nobel.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kPEs5uUUj7Gs"
      },
      "outputs": [],
      "source": [
        "#Example: Removing missing values\n",
        "Nobel=Nobel.dropna()\n",
        "Nobel.isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7NeKDsgj7Gs"
      },
      "source": [
        "#### 3.5.1 Label Binarizer\n",
        "The column 'Sex' only contains the two categories 'Male' and 'Female'. Since we removed the missing values none are unknown any more. One could have grouped together the missing values and set them to a third category 'Unknown'.\n",
        "By treating the values of 'Sex' as binary we can set either 'Male' or 'Female' to '0' and the other category to '1'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HwyetP3Jj7Gs"
      },
      "outputs": [],
      "source": [
        "#Example: Binarizing the entries for 'Male' and 'Female' to '0' and '1'\n",
        "lbNobel=Nobel.copy()\n",
        "lbNobel.Sex=label_binarize(lbNobel.Sex, classes=['Male','Female'])\n",
        "lbNobel.Sex.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7S3NALrvj7Gs"
      },
      "outputs": [],
      "source": [
        "#Example: Binarizing the entries for 'Male' and 'Female' to '1' and '0'\n",
        "lbNobel=Nobel.copy()\n",
        "lbNobel.Sex=label_binarize(lbNobel.Sex, classes=['Female','Male'])\n",
        "lbNobel.Sex.value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MnwMex4Jj7Gt"
      },
      "source": [
        "#### 3.5.2 LabelEncoder\n",
        "With LabelEncoder all the different categories in a column receive unique numbers. If there are many such categories, like for example in the columns 'Birth Country' and 'Organization Name', the reference of the generated numbers back to their meaning might be difficult to obtain. One way is to keep the original column and to add a new column with the converted numbers. Another one is to use the labelEncoder's inverse_transform() function.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "hXrZrxvbj7Gt"
      },
      "outputs": [],
      "source": [
        "#Example: Use of LabelEncoder to convert the categories in 'Organization Name' into a new column 'ON'\n",
        "le = LabelEncoder() #defines the labelEncoder\n",
        "nobCat=Nobel.copy()\n",
        "nobCat['ON']=le.fit_transform(nobCat['Organization Name']) #label encoder learns from the data and then transforms it\n",
        "nobCat.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "48lJoZ29j7Gt"
      },
      "outputs": [],
      "source": [
        "le.inverse_transform([14, 137, 159, 237])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0o4jTiNIj7Gt"
      },
      "source": [
        "#### 3.5.3 OrdinalEncoder\n",
        "The Ordinal Encoder is similar to the Label Encoder. The difference is that it allows to transform several columns at once, while Label Encoder only transfers one column at the time.\n",
        "\n",
        "We illustrate the use of Ordinal Encoder on the columns 'Category', and 'Birth Country':\n",
        "- the ordinal encoder returns data as Series and not as DataFrames. One can turn them into DataFrames by using the constructor.\n",
        "- The DataFrame constructor turns data into data frames with the column names indicated\n",
        "- Since we want to add new columns to the existing data we add them to the existing ones"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "WvJWq-jpj7Gt"
      },
      "outputs": [],
      "source": [
        "#Example: Use of Ordinal Encoder on 'Category' and 'Birth Country'\n",
        "oe = OrdinalEncoder() #defines the OrdinalEncoder\n",
        "nobCat.loc[:,['CN','BCN']]=pd.DataFrame(oe.fit_transform(nobCat.loc[:,['Category','Birth Country']]),columns=['CN','BCN'])\n",
        "nobCat.head(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dooFAFkj7Gt"
      },
      "source": [
        "#### 3.5.4 Dummy Variables\n",
        "Sometimes one would like to treat the categories (numerical values) in the column more explicitly, kind of like binary values with '1' meaning that the category is present and '0' that it is not present. With help of 'dummy variables', each category in the column is transfered into an own column. If there are many different categories this will lead to many more columns and to what is called a 'sparse matrix' where most of the elements in the table are '0'.\n",
        "We illustrate the use of 'dumy variables' for the column 'Sex'. Two new columns will be created 'Sex_Male' and 'Sex_Female. The function 'get_dummies()' automatically creates new columns with the combined name of the original column and the names of the categories, e.g. 'Sex' and 'Male' or 'Female' combined with a '_' between."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yos7ZNVzj7Gt"
      },
      "outputs": [],
      "source": [
        "#Example: Creating two 'dummy variables' for the column 'Sex'\n",
        "# using the function get_dummies()\n",
        "# see: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.get_dummies.html?highlight=get_dummies\n",
        "\n",
        "nobCat=pd.get_dummies(nobCat, columns=['Sex'])\n",
        "nobCat.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "booESDLXj7Gu"
      },
      "source": [
        "### Converting categorical data in the Life Expectancy data set\n",
        "Back to the converting of categorical data in the life Expectancy data set. There are only two columns with categorical data:\n",
        "- 'Country' contains the names of all the countries\n",
        "- 'Status' is a binary value that reads either 'Developing' or 'Developed'\n",
        "\n",
        "We will add a column 'Country_Nr' for the converted number per country and binarize the column 'Status' into a new column 'Developed' with '1' and '0'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w2tsrYd8j7Gu"
      },
      "outputs": [],
      "source": [
        "#Converting country names\n",
        "le = LabelEncoder() #defines the labelEncoder\n",
        "dfLE['Country_Nr']=le.fit_transform(dfLE['Country'])\n",
        "#Converting Status\n",
        "#make sure that 'Developed' becomes '1' and 'Developing' becomes '0'\n",
        "print(dfLE.Status.value_counts())\n",
        "dfLE.Status=label_binarize(dfLE.Status, classes=['Developing','Developed'])\n",
        "print(dfLE.Status.value_counts())\n",
        "# change the name of the column 'Status' to 'Developed'\n",
        "dfLE.rename(columns={'Status':'Developed'}, inplace='True')\n",
        "dfLE.head(4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0sZSgNxVj7Gu"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0TEbVZcj7Gu"
      },
      "source": [
        "### 3.6 Aggregation of data\n",
        "In the life expectancy data set, the data is organized after country and year. If one would like to compare different countries with one another one therefore needs to calculate averages over the different years.\n",
        "To group data and to calculate averages for a group is also known as data aggregation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "6nPrz1nfj7Gu"
      },
      "outputs": [],
      "source": [
        "#Example: We group the data after their country names and calculate the averages in every feature.\n",
        "#The result is stored in a new variable called LEAggregated \n",
        "LEAggregated=dfLE.groupby(['Country']).mean()\n",
        "LEAggregated.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekJWgQNsj7Gu"
      },
      "source": [
        "The groupby() function groups our data as wanted, but at the same time changed our index to the countries. This might be useful in a way, but we have not worked with indexes other than incremental numbers before. To avoid confusion,\n",
        "we therefore change the index back to the way they were before"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ad2Jidnaj7Gu"
      },
      "outputs": [],
      "source": [
        "LEAggregated2=LEAggregated.copy() #making a copy in order to keep the data frame with country-indexes\n",
        "LEAggregated=LEAggregated.reset_index()\n",
        "LEAggregated.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Na6s2ipcj7Gv"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_h_Q_xWj7Gv"
      },
      "source": [
        "## 4. Saving the Pre-Processed Data\n",
        "At the end of this tutorial we now have some pre-processed datasets that contain no missing data, have corrected entries and categories that are converted to numerical values.\n",
        "\n",
        "Before closing the tutorial, we want to save this data so that we can easely re-load it again once we continue working with it in the next tutorial.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uRamnil0j7Gv"
      },
      "source": [
        "### 4.1 Pickling and Unpickling Data\n",
        "Most programming languages allow to save variables with their values on hard-disks from where they easely can be re-loaded again. This is convinient, especially since the values otherwise are lost when exiting from the program and need to be recreated from scratch. \n",
        "In Python, the process of saving variables on disk is called 'pickling' and loading variables from disk is called 'unpickling'. All types of variables and data can be pickled and unpickled."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W4z420-qj7Gv"
      },
      "outputs": [],
      "source": [
        "#Example: Pickling the variables rawdata, dfLE,LEAggregated, LEAggregated2 into a file with the name \"Tutorial_T1_Results.pickle\"\n",
        "d_out=open(outputFilePath+\"Tutorial_T1_Results.pickle\",\"wb\") #\"wb\" defines the writing mode for the file\n",
        "pickle.dump([rawdata, dfLE,LEAggregated, LEAggregated2],d_out) #dump the content of the variables into the file\n",
        "d_out.close() # never forget to close an open file!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bykfdm_4j7Gv"
      },
      "source": [
        "The file with the name \"Tutorial_T1_Results.pickle\" should have been created in the same folder as this notebook.\n",
        "It is good practice whenever pickling variables to make sure that the variables can be unpickled! Use new variable-names when unpickling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EFIlP3qOj7Gv"
      },
      "outputs": [],
      "source": [
        "#Example: of unpickling the variables from the file with the name \"Tutorial_T1_Results.pickle\"\n",
        "d_in = open(outputFilePath+\"Tutorial_T1_Results.pickle\",\"rb\") # \"rb\" defines the reading mode for the file\n",
        "# using other variable names\n",
        "[rawdataX, dfLEX,LEAggregatedX, LEAggregated2X] = pickle.load(d_in)\n",
        "d_in.close() # never forget to close an open file!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qy3C13QNj7Gw"
      },
      "outputs": [],
      "source": [
        "# proof that it worked\n",
        "dfLEX.head(3)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nuYSlb7Lj7Gw"
      },
      "source": [
        "### 4.2 Saving the resulting dataset in excel\n",
        "\n",
        "In the case of data that is stored in DataFrames variables, it makes sense to save all of the data into an excel sheet as long as the dataset is not bigger than what Excel can manage. \n",
        "The advantage of having all the data in excel is that the functionality of excel can be used to analyze or visualize the data and that it is easy to load data from excel into DataFrames again.\n",
        "\n",
        "OBS: If you are using Google's Colab you will need to add the path \"/drive/My Drive/MA661E/\" before the filename. make sure to change \"MA661E\" to the name of your folder on your Google Drive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QCQdA-1sj7Gw"
      },
      "outputs": [],
      "source": [
        "#Example: Saving the pre-processed DataFrame dfLE in the excel file with the name \"dfLE.xlsx\" \n",
        "# into the excel-sheet dfLE.\n",
        "# see: https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html#io-excel-writer\n",
        "dfLE.to_excel(outputFilePath+'dfLE.xlsx', sheet_name='dfLE')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xb5AwDQ_j7Gw"
      },
      "outputs": [],
      "source": [
        "#Example: Saving several DataFrame variables (dfLE,LEAggregated, LEAggregated2) in the same excel-file on \n",
        "# different sheets you need an \"ExcelWriter\"\n",
        "with pd.ExcelWriter(outputFilePath+'Life Expectancy preprocessed data.xlsx') as writer:\n",
        "    dfLE.to_excel(writer, sheet_name='dfLE')\n",
        "    LEAggregated.to_excel(writer, sheet_name='LEAggregated')\n",
        "    LEAggregated2.to_excel(writer, sheet_name='LEAggregated-country-inx')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "Tutorial-T1-DataPreparation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}